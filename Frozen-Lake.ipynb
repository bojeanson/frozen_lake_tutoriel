{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel : Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONNEL : Fonction d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = {0: \"Gauche\", 1: \"Bas\", 2: \"Droite\", 3: \"Haut\"}\n",
    "\n",
    "def map_action(action_int):\n",
    "    return MAP.get(action_int, None)\n",
    "\n",
    "def my_render(env):\n",
    "    my_env = env\n",
    "    row, col = my_env.s // my_env.ncol, my_env.s % my_env.ncol\n",
    "    desc = my_env.desc.tolist()\n",
    "    desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "    desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "    print(\"\\nDernière action faite: {}\".format(map_action(my_env.lastaction)))\n",
    "    grid = \"|\" + \"|\\n|\".join(''.join(line) for line in desc) + \"|\"\n",
    "    grid = grid.replace(\"F\", \" \").replace(\"H\", \"X\")\n",
    "    print(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialisation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "   id=\"FrozenLakeNotSlippery-v0\",\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Nombre d'états dans l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Nombre d'actions possibles dans chaque état"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Initialisation de la Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((state_size, action_size))\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Nombre d'episode maximun (durée de l'entrainement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Taux d'apprentissage (ou *learning rate* -> alpha) et facteur d'actualisation (ou *discount factor* -> gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Initialisation d'epsilon (epsilon - greedy policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1.0\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY_RATE = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exécution de l'action dans l'environnement et modification de la reward par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action, env):\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    # Reward function\n",
    "    # If new_state is a hole\n",
    "    if new_state in [5, 7, 11, 12]:\n",
    "        reward = -1\n",
    "    # else if new_state is the arrival\n",
    "    elif new_state == 15:\n",
    "        reward = 1\n",
    "    # else penalize search\n",
    "    else:\n",
    "        reward = -0.01\n",
    "    return new_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boucle principale d'apprentissage avec MAX_EPISODES episodes\n",
    "### Boucle *tant que* imbriquée pour le déroulement de chaque episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(MAX_EPISODES):\n",
    "\n",
    "    S = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        # ETAPE 1\n",
    "        if random.uniform(0, 1) < EPSILON:\n",
    "            A = env.action_space.sample()\n",
    "        else:\n",
    "            A = np.argmax(Q_table[S, :])\n",
    "        # ETAPE 2\n",
    "        S_, R, done, info = take_action(A, env)\n",
    "        # ETAPE 3\n",
    "        q_predict = Q_table[S, A]\n",
    "        if done:\n",
    "            q_target = R\n",
    "        else:\n",
    "            q_target = R + GAMMA * np.max(Q_table[S_, :])\n",
    "        Q_table[S, A] += ALPHA * (q_target - q_predict)\n",
    "        total_rewards += R\n",
    "        S = S_\n",
    " \n",
    "        # Fonction d'affichage par défaut\n",
    "        #env.render()\n",
    "        # Fonction d'affichage \"custom\" / plus lisible\n",
    "        my_render(env)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Epsilon decay\n",
    "    EPSILON = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * episode)\n",
    "    rewards.append(total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
